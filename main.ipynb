{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd38936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fm-pc-lt321/Desktop/suyesh/coding Mountain/fina/codingMountain/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Processing: Operation_Overload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fm-pc-lt321/Desktop/suyesh/coding Mountain/fina/codingMountain/.venv/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "Created a chunk of size 747, which is longer than the specified 500\n",
      "Created a chunk of size 1734, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Created 422 chunks for Operation_Overload\n",
      "âœ… Neo4j connection successful\n",
      "ðŸ’¾ Saved clean text -> /home/fm-pc-lt321/Desktop/suyesh/coding Mountain/fina/codingMountain/clean/Operation_Overload_clean.txt\n",
      "ðŸ’¾ Saved indicators -> /home/fm-pc-lt321/Desktop/suyesh/coding Mountain/fina/codingMountain/raw/Operation_Overload_indicators.json\n",
      "âœ… Processed 'Operation_Overload' (422 chunks, 90 pages)\n",
      "ðŸ”„ Processing: Storm-1516 Technical Report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1084, which is longer than the specified 500\n",
      "Created a chunk of size 634, which is longer than the specified 500\n",
      "Created a chunk of size 963, which is longer than the specified 500\n",
      "Created a chunk of size 519, which is longer than the specified 500\n",
      "Created a chunk of size 630, which is longer than the specified 500\n",
      "Created a chunk of size 728, which is longer than the specified 500\n",
      "Created a chunk of size 1012, which is longer than the specified 500\n",
      "Created a chunk of size 764, which is longer than the specified 500\n",
      "Created a chunk of size 535, which is longer than the specified 500\n",
      "Created a chunk of size 530, which is longer than the specified 500\n",
      "Created a chunk of size 727, which is longer than the specified 500\n",
      "Created a chunk of size 668, which is longer than the specified 500\n",
      "Created a chunk of size 663, which is longer than the specified 500\n",
      "Created a chunk of size 516, which is longer than the specified 500\n",
      "Created a chunk of size 758, which is longer than the specified 500\n",
      "Created a chunk of size 528, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n",
      "Created a chunk of size 550, which is longer than the specified 500\n",
      "Created a chunk of size 611, which is longer than the specified 500\n",
      "Created a chunk of size 566, which is longer than the specified 500\n",
      "Created a chunk of size 501, which is longer than the specified 500\n",
      "Created a chunk of size 649, which is longer than the specified 500\n",
      "Created a chunk of size 524, which is longer than the specified 500\n",
      "Created a chunk of size 569, which is longer than the specified 500\n",
      "Created a chunk of size 514, which is longer than the specified 500\n",
      "Created a chunk of size 611, which is longer than the specified 500\n",
      "Created a chunk of size 826, which is longer than the specified 500\n",
      "Created a chunk of size 695, which is longer than the specified 500\n",
      "Created a chunk of size 789, which is longer than the specified 500\n",
      "Created a chunk of size 817, which is longer than the specified 500\n",
      "Created a chunk of size 835, which is longer than the specified 500\n",
      "Created a chunk of size 758, which is longer than the specified 500\n",
      "Created a chunk of size 4225, which is longer than the specified 500\n",
      "Created a chunk of size 2014, which is longer than the specified 500\n",
      "Created a chunk of size 668, which is longer than the specified 500\n",
      "Created a chunk of size 516, which is longer than the specified 500\n",
      "Created a chunk of size 739, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Created 337 chunks for Storm-1516 Technical Report\n",
      "âœ… Neo4j connection successful\n",
      "ðŸ’¾ Saved clean text -> /home/fm-pc-lt321/Desktop/suyesh/coding Mountain/fina/codingMountain/clean/Storm-1516 Technical Report_clean.txt\n",
      "ðŸ’¾ Saved indicators -> /home/fm-pc-lt321/Desktop/suyesh/coding Mountain/fina/codingMountain/raw/Storm-1516 Technical Report_indicators.json\n",
      "âœ… Processed 'Storm-1516 Technical Report' (337 chunks, 39 pages)\n",
      "ðŸ”„ Processing: DoppelgÃ¤nger Campaign Report\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 1037, which is longer than the specified 500\n",
      "Created a chunk of size 532, which is longer than the specified 500\n",
      "Created a chunk of size 766, which is longer than the specified 500\n",
      "Created a chunk of size 744, which is longer than the specified 500\n",
      "Created a chunk of size 673, which is longer than the specified 500\n",
      "Created a chunk of size 514, which is longer than the specified 500\n",
      "Created a chunk of size 1360, which is longer than the specified 500\n",
      "Created a chunk of size 868, which is longer than the specified 500\n",
      "Created a chunk of size 1136, which is longer than the specified 500\n",
      "Created a chunk of size 951, which is longer than the specified 500\n",
      "Created a chunk of size 1374, which is longer than the specified 500\n",
      "Created a chunk of size 624, which is longer than the specified 500\n",
      "Created a chunk of size 831, which is longer than the specified 500\n",
      "Created a chunk of size 1233, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Created 175 chunks for DoppelgÃ¤nger Campaign Report\n",
      "âœ… Neo4j connection successful\n",
      "ðŸ’¾ Saved clean text -> /home/fm-pc-lt321/Desktop/suyesh/coding Mountain/fina/codingMountain/clean/DoppelgÃ¤nger Campaign Report_clean.txt\n",
      "ðŸ’¾ Saved indicators -> /home/fm-pc-lt321/Desktop/suyesh/coding Mountain/fina/codingMountain/raw/DoppelgÃ¤nger Campaign Report_indicators.json\n",
      "âœ… Processed 'DoppelgÃ¤nger Campaign Report' (175 chunks, 28 pages)\n",
      "ðŸ“Š Processing complete: 3/3 PDFs processed successfully\n"
     ]
    }
   ],
   "source": [
    "# main.py - Usage example\n",
    "from pathlib import Path\n",
    "from utils import DatabaseConnections\n",
    "from preprocessing import PDFProcessor\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function demonstrating the refactored PDF processing.\"\"\"\n",
    "    # Initialize processor\n",
    "    processor = PDFProcessor()\n",
    "    \n",
    "    # Setup database connections\n",
    "    db_conn = DatabaseConnections()\n",
    "    \n",
    "    try:\n",
    "        # PostgreSQL connection\n",
    "        postgres_conn = db_conn.connect_postgres()\n",
    "        postgres_cursor = postgres_conn.cursor()\n",
    "        \n",
    "        # Neo4j connection (optional)\n",
    "        neo4j_driver = db_conn.connect_neo4j()\n",
    "        \n",
    "        # Process PDFs from config\n",
    "        processor.process_multiple_pdfs(postgres_cursor, neo4j_driver)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during processing: {e}\")\n",
    "    finally:\n",
    "        # Clean up connections\n",
    "        if 'postgres_cursor' in locals():\n",
    "            postgres_cursor.close()\n",
    "        if 'postgres_conn' in locals():\n",
    "            postgres_conn.close()\n",
    "        if 'neo4j_driver' in locals():\n",
    "            neo4j_driver.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29779a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
